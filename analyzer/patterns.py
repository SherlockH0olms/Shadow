"""
ShadowHunter AI - Pattern Detection Engine
Detects AI-generated code patterns and malware signatures
"""

import ast
import re
import math
from typing import Dict, List, Tuple
from collections import Counter


class AICodePatternDetector:
    """
    Advanced pattern detector for AI-generated malware
    Focuses on DeepSeek, GPT-4, and Claude signatures
    """

    def __init__(self):
        # LLM-specific code signatures
        self.llm_signatures = {
            "deepseek": [
                r"CRYSTALS-Kyber",
                r"quantum[_\s]*encrypt",
                r"np\.random\.seed\(os\.urandom",
                r"THE_AUTHOR_IS_",
                r"post[_\s]*quantum",
                r"lattice[_\s]*based",
            ],
            "gpt4": [
                r"import\s+ctypes.*windll",
                r"NtAllocateVirtualMemory",
                r"syscall.*evas",
                r"polymorphic.*engine",
                r"shellcode.*generation",
                r"kernel32\.VirtualProtect",
            ],
            "claude": [
                r"async\s+def.*inject",
                r"multi[_\s]*agent.*orchestrat",
                r"# CRITICAL.*WARNING",
                r"ethical.*considerations",
                r"# Anthropic Claude",
            ],
            "generic_ai": [
                r"AI[_\s]*generated",
                r"LLM[_\s]*powered",
                r"GPT[_\s]*assisted",
                r"# Generated by",
                r"# Auto-generated code",
            ]
        }

        # Evasion and obfuscation techniques
        self.evasion_patterns = [
            r"vssadmin\s+delete\s+shadows",  # Ransomware
            r"wevtutil\s+cl",  # Log clearing
            r"\\\\\.\\PhysicalDrive",  # Direct disk access
            r"bios.*persist",  # BIOS persistence
            r"NtCreateThreadEx",  # Thread injection
            r"ghost.*inject",  # Process hollowing
            r"amsi.*bypass",  # AMSI bypass
            r"etw.*patch",  # ETW patching
            r"reflective.*dll",  # Reflective DLL injection
            r"\\x[0-9a-fA-F]{2}",  # Hex shellcode
        ]

        # Malicious function indicators
        self.malicious_functions = [
            "CreateRemoteThread",
            "WriteProcessMemory",
            "VirtualAllocEx",
            "SetWindowsHookEx",
            "keybd_event",
            "GetAsyncKeyState",
            "CryptEncrypt",
            "RtlMoveMemory",
        ]

        # Suspicious imports
        self.suspicious_imports = [
            "ctypes",
            "winreg",
            "wmi",
            "pywinauto",
            "pyautogui",
            "keyboard",
            "pynput",
            "scapy",
            "paramiko",
        ]

    def analyze(self, code: str) -> Dict:
        """
        Comprehensive code analysis pipeline

        Args:
            code: Source code to analyze

        Returns:
            Analysis results with detection verdict
        """

        results = {
            "is_ai_generated": False,
            "confidence": 0.0,
            "llm_source": "unknown",
            "detected_patterns": [],
            "risk_score": 0,
            "entropy": 0.0,
            "obfuscation_level": "none",
            "evasion_techniques": [],
            "malicious_indicators": [],
            "ast_analysis": {},
        }

        # 1. LLM Signature Detection
        llm_matches = self._detect_llm_signatures(code)
        if llm_matches:
            results["is_ai_generated"] = True
            results["llm_source"] = llm_matches["source"]
            results["detected_patterns"] = llm_matches["patterns"]
            results["confidence"] += 0.4

        # 2. Entropy Analysis (obfuscation detection)
        entropy = self._calculate_entropy(code)
        results["entropy"] = entropy
        if entropy > 7.5:  # High entropy = likely obfuscated
            results["confidence"] += 0.2
            results["obfuscation_level"] = "high"
        elif entropy > 6.5:
            results["confidence"] += 0.1
            results["obfuscation_level"] = "medium"

        # 3. AST-based Analysis
        ast_results = self._analyze_ast(code)
        results["ast_analysis"] = ast_results
        if ast_results.get("complexity_score", 0) > 100:
            results["confidence"] += 0.15

        # 4. Evasion Technique Detection
        evasion = self._detect_evasion(code)
        results["evasion_techniques"] = evasion
        if len(evasion) > 0:
            results["confidence"] += 0.25
            results["risk_score"] = min(100, len(evasion) * 20)

        # 5. Malicious Function Detection
        malicious = self._detect_malicious_functions(code)
        results["malicious_indicators"] = malicious
        if len(malicious) > 0:
            results["confidence"] += 0.15
            results["risk_score"] += len(malicious) * 10

        # 6. String Analysis
        string_analysis = self._analyze_strings(code)
        results["string_analysis"] = string_analysis

        # Final confidence normalization
        results["confidence"] = min(1.0, results["confidence"])
        results["risk_score"] = min(100, results["risk_score"])

        return results

    def _detect_llm_signatures(self, code: str) -> Dict:
        """
        Detect LLM-specific code patterns
        """

        for llm, patterns in self.llm_signatures.items():
            matches = []
            for pattern in patterns:
                if re.search(pattern, code, re.IGNORECASE | re.MULTILINE):
                    matches.append(pattern)

            if matches:
                return {
                    "source": llm,
                    "patterns": matches,
                    "match_count": len(matches)
                }

        return None

    def _calculate_entropy(self, data: str) -> float:
        """
        Calculate Shannon entropy for obfuscation detection
        """

        if not data:
            return 0.0

        entropy = 0.0
        counter = Counter(data)
        length = len(data)

        for count in counter.values():
            probability = count / length
            entropy -= probability * math.log2(probability)

        return round(entropy, 2)

    def _analyze_ast(self, code: str) -> Dict:
        """
        Abstract Syntax Tree analysis for code complexity
        """

        try:
            tree = ast.parse(code)

            node_counts = {
                "imports": 0,
                "functions": 0,
                "classes": 0,
                "suspicious_functions": 0,
                "obfuscated_strings": 0,
                "loops": 0,
                "conditionals": 0,
            }

            suspicious_names = [
                "inject", "hook", "bypass", "evade", "payload",
                "exploit", "shellcode", "backdoor", "rootkit"
            ]

            for node in ast.walk(tree):
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    node_counts["imports"] += 1

                elif isinstance(node, ast.FunctionDef):
                    node_counts["functions"] += 1
                    # Check for suspicious function names
                    if any(x in node.name.lower() for x in suspicious_names):
                        node_counts["suspicious_functions"] += 1

                elif isinstance(node, ast.ClassDef):
                    node_counts["classes"] += 1

                elif isinstance(node, (ast.For, ast.While)):
                    node_counts["loops"] += 1

                elif isinstance(node, ast.If):
                    node_counts["conditionals"] += 1

                elif isinstance(node, ast.Str):
                    # Check for obfuscated strings
                    if len(node.s) > 100 and self._calculate_entropy(node.s) > 7.0:
                        node_counts["obfuscated_strings"] += 1

            # Calculate complexity score
            complexity = (
                node_counts["imports"] * 2 +
                node_counts["functions"] * 5 +
                node_counts["suspicious_functions"] * 15 +
                node_counts["obfuscated_strings"] * 20 +
                node_counts["loops"] * 3 +
                node_counts["conditionals"] * 2
            )

            return {
                **node_counts,
                "complexity_score": complexity
            }

        except SyntaxError:
            return {
                "error": "Invalid Python syntax",
                "complexity_score": 0
            }

    def _detect_evasion(self, code: str) -> List[str]:
        """
        Detect anti-analysis and evasion techniques
        """

        detected = []

        for pattern in self.evasion_patterns:
            if re.search(pattern, code, re.IGNORECASE):
                detected.append(pattern.replace("\\", ""))

        # Additional heuristic checks
        if "os.urandom" in code and "np.random" in code:
            detected.append("Polymorphic entropy generation")

        if "ctypes" in code and "kernel32" in code:
            detected.append("Direct Windows API calls")

        if "exec(" in code or "eval(" in code:
            detected.append("Dynamic code execution")

        if re.search(r"base64\.b64decode", code):
            detected.append("Base64 encoded payload")

        if re.search(r"\\x[0-9a-f]{2}", code):
            detected.append("Hex-encoded shellcode")

        return detected

    def _detect_malicious_functions(self, code: str) -> List[str]:
        """
        Detect known malicious function calls
        """

        detected = []

        for func in self.malicious_functions:
            if func in code:
                detected.append(func)

        return detected

    def _analyze_strings(self, code: str) -> Dict:
        """
        Analyze strings for suspicious content
        """

        # Extract strings
        string_pattern = r'["\']([^"\']{10,})["\']'
        strings = re.findall(string_pattern, code)

        suspicious_keywords = [
            "password", "credential", "token", "api_key",
            "payload", "exploit", "backdoor", "c2", "command"
        ]

        suspicious_strings = []
        for s in strings:
            if any(keyword in s.lower() for keyword in suspicious_keywords):
                suspicious_strings.append(s[:50])  # Truncate for safety

        return {
            "total_strings": len(strings),
            "suspicious_strings": suspicious_strings[:10],  # Limit output
            "avg_entropy": round(
                sum(self._calculate_entropy(s) for s in strings) / len(strings), 2
            ) if strings else 0.0
        }


# Standalone test function
if __name__ == "__main__":
    detector = AICodePatternDetector()

    # Test sample
    test_code = """
import ctypes
import os
import numpy as np

# CRYSTALS-Kyber quantum encryption
def quantum_encrypt(data):
    seed = os.urandom(32)
    np.random.seed(int.from_bytes(seed, 'big'))
    return encrypted_data

# Ghost injection technique
def ghost_inject():
    kernel32 = ctypes.windll.kernel32
    NtAllocateVirtualMemory()
    """

    result = detector.analyze(test_code)
    print("Analysis Result:")
    print(f"  AI Generated: {result['is_ai_generated']}")
    print(f"  LLM Source: {result['llm_source']}")
    print(f"  Confidence: {result['confidence']:.2%}")
    print(f"  Risk Score: {result['risk_score']}")
