#!/usr/bin/env python3
"""
ShadowHunter AI - Pattern Detection Engine
Detects AI-generated code patterns and malware signatures

Production-ready implementation with comprehensive error handling
"""

import ast
import re
import math
import logging
from typing import Dict, List, Optional, Set
from collections import Counter

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AICodePatternDetector:
    """
    Advanced pattern detector for AI-generated malware.
    Detects signatures from DeepSeek, GPT-4, Claude, and other LLMs.
    """

    def __init__(self):
        """Initialize detector with pattern databases."""
        
        # LLM-specific code signatures
        self.llm_signatures = {
            "deepseek": [
                r"CRYSTALS-Kyber",
                r"quantum[_\s]*encrypt",
                r"np\.random\.seed\(os\.urandom",
                r"THE_AUTHOR_IS_",
                r"post[_\s]*quantum",
                r"lattice[_\s]*based",
                r"kyber\d+",
                r"dilithium",
            ],
            "gpt4": [
                r"import\s+ctypes.*windll",
                r"NtAllocateVirtualMemory",
                r"syscall.*evas",
                r"polymorphic.*engine",
                r"shellcode.*generation",
                r"kernel32\.VirtualProtect",
                r"ntdll\.dll",
                r"# OpenAI GPT",
            ],
            "claude": [
                r"async\s+def.*inject",
                r"multi[_\s]*agent.*orchestrat",
                r"# CRITICAL.*WARNING",
                r"ethical.*considerations",
                r"# Anthropic Claude",
                r"# AI Assistant:",
            ],
            "generic_ai": [
                r"AI[_\s]*generated",
                r"LLM[_\s]*powered",
                r"GPT[_\s]*assisted",
                r"# Generated by",
                r"# Auto-generated code",
                r"# AI-created",
            ]
        }

        # Evasion and obfuscation techniques
        self.evasion_patterns = [
            r"vssadmin\s+delete\s+shadows",
            r"wevtutil\s+cl",
            r"\\\\\.\\PhysicalDrive",
            r"bios.*persist",
            r"NtCreateThreadEx",
            r"ghost.*inject",
            r"amsi.*bypass",
            r"etw.*patch",
            r"reflective.*dll",
            r"\\\\x[0-9a-fA-F]{2}",
        ]

        # Malicious function indicators
        self.malicious_functions = [
            "CreateRemoteThread",
            "WriteProcessMemory",
            "VirtualAllocEx",
            "SetWindowsHookEx",
            "keybd_event",
            "GetAsyncKeyState",
            "CryptEncrypt",
            "RtlMoveMemory",
            "LoadLibraryA",
            "GetProcAddress",
        ]

        # Suspicious imports
        self.suspicious_imports = [
            "ctypes",
            "winreg",
            "wmi",
            "pywinauto",
            "pyautogui",
            "keyboard",
            "pynput",
            "scapy",
            "paramiko",
        ]

    def analyze(self, code: str) -> Dict:
        """
        Comprehensive code analysis pipeline.

        Args:
            code: Source code to analyze (string)

        Returns:
            Dict containing analysis results with detection verdict
        """
        if not code or not isinstance(code, str):
            logger.error("Invalid input: code must be non-empty string")
            return self._empty_result()

        results = {
            "is_ai_generated": False,
            "confidence": 0.0,
            "llm_source": "unknown",
            "detected_patterns": [],
            "risk_score": 0,
            "entropy": 0.0,
            "obfuscation_level": "none",
            "evasion_techniques": [],
            "malicious_indicators": [],
            "suspicious_imports": [],
            "ast_analysis": {},
            "string_analysis": {},
        }

        try:
            # 1. LLM Signature Detection
            llm_matches = self._detect_llm_signatures(code)
            if llm_matches:
                results["is_ai_generated"] = True
                results["llm_source"] = llm_matches["source"]
                results["detected_patterns"] = llm_matches["patterns"]
                results["confidence"] += 0.4
                logger.info(f"LLM signature detected: {llm_matches['source']}")

            # 2. Entropy Analysis
            entropy = self._calculate_entropy(code)
            results["entropy"] = entropy
            if entropy > 7.5:
                results["confidence"] += 0.2
                results["obfuscation_level"] = "high"
            elif entropy > 6.5:
                results["confidence"] += 0.1
                results["obfuscation_level"] = "medium"

            # 3. AST Analysis
            ast_results = self._analyze_ast(code)
            results["ast_analysis"] = ast_results
            if ast_results.get("complexity_score", 0) > 100:
                results["confidence"] += 0.15

            # 4. Evasion Techniques
            evasion = self._detect_evasion(code)
            results["evasion_techniques"] = evasion
            if evasion:
                results["confidence"] += 0.25
                results["risk_score"] = min(100, len(evasion) * 20)

            # 5. Malicious Functions
            malicious = self._detect_malicious_functions(code)
            results["malicious_indicators"] = malicious
            if malicious:
                results["confidence"] += 0.15
                results["risk_score"] += len(malicious) * 10

            # 6. Suspicious Imports
            suspicious_imports = self._detect_suspicious_imports(code)
            results["suspicious_imports"] = suspicious_imports
            if suspicious_imports:
                results["confidence"] += 0.05

            # 7. String Analysis
            string_analysis = self._analyze_strings(code)
            results["string_analysis"] = string_analysis

            # Final normalization
            results["confidence"] = min(1.0, results["confidence"])
            results["risk_score"] = min(100, results["risk_score"])

            logger.info(f"Analysis complete - Confidence: {results['confidence']:.2%}")

        except Exception as e:
            logger.error(f"Analysis error: {str(e)}")
            results["error"] = str(e)

        return results

    def _detect_llm_signatures(self, code: str) -> Optional[Dict]:
        """Detect LLM-specific code patterns."""
        for llm, patterns in self.llm_signatures.items():
            matches = []
            for pattern in patterns:
                try:
                    if re.search(pattern, code, re.IGNORECASE | re.MULTILINE):
                        matches.append(pattern)
                except re.error as e:
                    logger.warning(f"Regex error for pattern {pattern}: {e}")
                    continue

            if matches:
                return {
                    "source": llm,
                    "patterns": matches,
                    "match_count": len(matches)
                }
        return None

    def _calculate_entropy(self, data: str) -> float:
        """Calculate Shannon entropy for obfuscation detection."""
        if not data:
            return 0.0

        try:
            counter = Counter(data)
            length = len(data)
            entropy = 0.0

            for count in counter.values():
                probability = count / length
                entropy -= probability * math.log2(probability)

            return round(entropy, 2)
        except Exception as e:
            logger.warning(f"Entropy calculation error: {e}")
            return 0.0

    def _analyze_ast(self, code: str) -> Dict:
        """Abstract Syntax Tree analysis for code complexity."""
        try:
            tree = ast.parse(code)
            
            node_counts = {
                "imports": 0,
                "functions": 0,
                "classes": 0,
                "suspicious_functions": 0,
                "obfuscated_strings": 0,
                "loops": 0,
                "conditionals": 0,
            }

            suspicious_names = {
                "inject", "hook", "bypass", "evade", "payload",
                "exploit", "shellcode", "backdoor", "rootkit", "trojan"
            }

            for node in ast.walk(tree):
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    node_counts["imports"] += 1

                elif isinstance(node, ast.FunctionDef):
                    node_counts["functions"] += 1
                    if any(name in node.name.lower() for name in suspicious_names):
                        node_counts["suspicious_functions"] += 1

                elif isinstance(node, ast.ClassDef):
                    node_counts["classes"] += 1

                elif isinstance(node, (ast.For, ast.While)):
                    node_counts["loops"] += 1

                elif isinstance(node, ast.If):
                    node_counts["conditionals"] += 1

                # Python 3.8+ compatible string detection
                elif isinstance(node, ast.Constant) and isinstance(node.value, str):
                    if len(node.value) > 100 and self._calculate_entropy(node.value) > 7.0:
                        node_counts["obfuscated_strings"] += 1

            # Calculate complexity score
            complexity = (
                node_counts["imports"] * 2 +
                node_counts["functions"] * 5 +
                node_counts["suspicious_functions"] * 15 +
                node_counts["obfuscated_strings"] * 20 +
                node_counts["loops"] * 3 +
                node_counts["conditionals"] * 2
            )

            return {
                **node_counts,
                "complexity_score": complexity
            }

        except SyntaxError as e:
            logger.warning(f"Syntax error in code analysis: {e}")
            return {
                "error": "Invalid Python syntax",
                "complexity_score": 0
            }
        except Exception as e:
            logger.error(f"AST analysis error: {e}")
            return {
                "error": str(e),
                "complexity_score": 0
            }

    def _detect_evasion(self, code: str) -> List[str]:
        """Detect anti-analysis and evasion techniques."""
        detected = []

        for pattern in self.evasion_patterns:
            try:
                if re.search(pattern, code, re.IGNORECASE):
                    clean_pattern = pattern.replace("\\", "")
                    detected.append(clean_pattern)
            except re.error:
                continue

        # Heuristic checks
        if "os.urandom" in code and "random" in code:
            detected.append("Polymorphic entropy generation")

        if "ctypes" in code and "kernel32" in code:
            detected.append("Direct Windows API calls")

        if "exec(" in code or "eval(" in code:
            detected.append("Dynamic code execution")

        if re.search(r"base64\.b64decode", code):
            detected.append("Base64 encoded payload")

        return list(set(detected))  # Remove duplicates

    def _detect_malicious_functions(self, code: str) -> List[str]:
        """Detect known malicious function calls."""
        detected = []
        for func in self.malicious_functions:
            if func in code:
                detected.append(func)
        return detected

    def _detect_suspicious_imports(self, code: str) -> List[str]:
        """Detect suspicious import statements."""
        detected = []
        for imp in self.suspicious_imports:
            if re.search(rf"\bimport\s+{imp}\b|\bfrom\s+{imp}\b", code):
                detected.append(imp)
        return detected

    def _analyze_strings(self, code: str) -> Dict:
        """Analyze strings for suspicious content."""
        try:
            string_pattern = r'["\']([^"\'>]{10,})["\']'
            strings = re.findall(string_pattern, code)

            suspicious_keywords = {
                "password", "credential", "token", "api_key", "secret",
                "payload", "exploit", "backdoor", "c2", "command", "ransomware"
            }

            suspicious_strings = []
            for s in strings:
                if any(keyword in s.lower() for keyword in suspicious_keywords):
                    suspicious_strings.append(s[:50])

            avg_entropy = 0.0
            if strings:
                avg_entropy = sum(self._calculate_entropy(s) for s in strings) / len(strings)

            return {
                "total_strings": len(strings),
                "suspicious_strings": suspicious_strings[:10],
                "avg_entropy": round(avg_entropy, 2)
            }
        except Exception as e:
            logger.warning(f"String analysis error: {e}")
            return {
                "total_strings": 0,
                "suspicious_strings": [],
                "avg_entropy": 0.0
            }

    def _empty_result(self) -> Dict:
        """Return empty result structure."""
        return {
            "is_ai_generated": False,
            "confidence": 0.0,
            "llm_source": "unknown",
            "detected_patterns": [],
            "risk_score": 0,
            "entropy": 0.0,
            "obfuscation_level": "none",
            "evasion_techniques": [],
            "malicious_indicators": [],
            "suspicious_imports": [],
            "ast_analysis": {},
            "string_analysis": {},
            "error": "Invalid input"
        }


if __name__ == "__main__":
    # Test the detector
    detector = AICodePatternDetector()

    test_code = """
import ctypes
import os
import numpy as np

# CRYSTALS-Kyber quantum encryption
def quantum_encrypt(data):
    seed = os.urandom(32)
    np.random.seed(int.from_bytes(seed, 'big'))
    return encrypted_data

# Ghost injection technique
def ghost_inject():
    kernel32 = ctypes.windll.kernel32
    CreateRemoteThread()
    """

    print("\n" + "="*60)
    print("ShadowHunter AI - Pattern Detector Test")
    print("="*60)
    
    result = detector.analyze(test_code)
    
    print(f"\nAI Generated: {result['is_ai_generated']}")
    print(f"LLM Source: {result['llm_source']}")
    print(f"Confidence: {result['confidence']:.2%}")
    print(f"Risk Score: {result['risk_score']}/100")
    print(f"Entropy: {result['entropy']}")
    print(f"Obfuscation Level: {result['obfuscation_level']}")
    
    if result['detected_patterns']:
        print(f"\nDetected Patterns: {len(result['detected_patterns'])}")
        for pattern in result['detected_patterns'][:3]:
            print(f"  - {pattern}")
    
    if result['evasion_techniques']:
        print(f"\nEvasion Techniques: {len(result['evasion_techniques'])}")
        for tech in result['evasion_techniques']:
            print(f"  - {tech}")
    
    print("\n" + "="*60)
